{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training:\n",
    "# 1) Load all data from preprocessing (training/test splits, etc)\n",
    "# 2) Begin Training Models\n",
    "    #  a) Decision Tree\n",
    "    #  b) Naive Bayes\n",
    "    #  c) Logistic Regression\n",
    "    #  d) SVM\n",
    "# 3) Testing Models\n",
    "# 4) New Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from Models import ModelUtil\n",
    "from Data import Preprocessing, DataUtil\n",
    "from Visualization import VisualUtil, batch_image_to_excel\n",
    "from Logs import logging as logs\n",
    "\n",
    "import importlib\n",
    "import configparser\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('Data//config.ini')\n",
    "\n",
    "importlib.reload(Preprocessing)\n",
    "importlib.reload(ModelUtil)\n",
    "importlib.reload(VisualUtil)\n",
    "importlib.reload(batch_image_to_excel)\n",
    "importlib.reload(logs)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load all data from preprocessing \n",
    "importlib.reload(Preprocessing)\n",
    "newprocessing = 'True' in config['DATA']['USE_NEW_PREPROCESSING']\n",
    "infieldDataFrame, outfieldDataFrame = Preprocessing.dataFiltering([], newprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outfieldDataFrame.head)\n",
    "print(infieldDataFrame.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of this is mapping the strings to numbers for both infieldDataFrame and outfieldDataFrame so that the correlation matrix can be computed\n",
    "# This can most likely be moved to a method in the logging.py file\n",
    "infieldDF4Matrix = infieldDataFrame.copy()\n",
    "outfieldDF4Matrix = outfieldDataFrame.copy()\n",
    "strColumns = [] \n",
    "for cName in outfieldDF4Matrix.columns:\n",
    "    if(str(outfieldDF4Matrix[cName].dtype) in 'object'):\n",
    "        strColumns.append(cName)\n",
    "rValueDict = {}\n",
    "for cName in strColumns:\n",
    "    i = 0\n",
    "    infieldUniques = infieldDF4Matrix[cName].unique()\n",
    "    for rValue in infieldUniques:\n",
    "        rValueDict.update({rValue:i})\n",
    "        i+=1\n",
    "    infieldDF4Matrix[cName] = infieldDF4Matrix[cName].map(rValueDict)\n",
    "    uniqueVals = [x for x in outfieldDF4Matrix[cName].unique() if x not in infieldUniques]\n",
    "    for rValue in uniqueVals: \n",
    "        rValueDict.update({rValue:i})\n",
    "        i+=1\n",
    "    outfieldDF4Matrix[cName] = outfieldDF4Matrix[cName].map(rValueDict)\n",
    "infieldDF4Matrix = infieldDF4Matrix.replace(np.nan, 0)\n",
    "infieldDF4Matrix = infieldDF4Matrix.replace('', 0)\n",
    "outfieldDF4Matrix = outfieldDF4Matrix.replace(np.nan, 0)\n",
    "outfieldDF4Matrix = outfieldDF4Matrix.replace('', 0)\n",
    "\n",
    "# Correlation does not imply causation.\n",
    "# -1 means that the 2 variables have an inverse linear relationship: when X increases, Y decreases\n",
    "# 0 means no linear correlation between X and Y\n",
    "# 1 means that the 2 variables have a linear relationship: when X increases, Y increases too.\n",
    "infieldcorrmatrix = infieldDF4Matrix.corr()\n",
    "outfieldcorrmatrix = outfieldDF4Matrix.corr()\n",
    "if (config['LOGGING']['Excel'] == 'True'):\n",
    "    logs.writeToExcelSheet(infieldcorrmatrix, \"Infield Correlation Matrix\")\n",
    "    logs.writeToExcelSheet(outfieldcorrmatrix, \"Outfield Correlation Matrix\")\n",
    "if (config['LOGGING']['Debug'] == 'True'):\n",
    "    print(infieldcorrmatrix)\n",
    "    print(outfieldcorrmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(logs)\n",
    "importlib.reload(ModelUtil)\n",
    "# 2) Trains all Models and exports all data to an Excel Sheet\n",
    "max_depth = 50\n",
    "max_features = 30\n",
    "max_leaf_nodes = 150\n",
    "# could also add ways to change it for these hyperparams below for other models\n",
    "var_smoothing = 1e-9\n",
    "lr = 0.8\n",
    "e = 100\n",
    "rC = 1\n",
    "kernel='linear'\n",
    "degree= 1\n",
    "gamma= 'scale'\n",
    "coef0= 0.0\n",
    "xoTrain, xoTest, yoTrain, yoTest = ModelUtil.modelDataSplitting(outfieldDataFrame, 11, 0.25,'OutfieldTrainingFilter', \"Outfield\")\n",
    "print(yoTrain.head)\n",
    "OutfielddtOutput = ModelUtil.runDT(xoTrain, yoTrain, xoTest, yoTest, max_depth, max_features, max_leaf_nodes, \"Outfield\")\n",
    "OutfieldnbOutput = ModelUtil.runNB(xoTrain, yoTrain, xoTest, yoTest, var_smoothing, \"Outfield\")\n",
    "OutfieldlogRegOutput = ModelUtil.runLogReg(xoTrain, yoTrain, xoTest, yoTest, lr, e, \"Outfield\")\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = ModelUtil.modelDataSplitting(infieldDataFrame, 11, 0.25,'InfieldTrainingFilter', \"Infield\")\n",
    "dtOutput = ModelUtil.runDT(xTrain, yTrain, xTest, yTest, max_depth, max_features, max_leaf_nodes, \"Infield\")\n",
    "nbOutput = ModelUtil.runNB(xTrain, yTrain, xTest, yTest, var_smoothing, \"Infield\")\n",
    "logRegOutput = ModelUtil.runLogReg(xTrain, yTrain, xTest, yTest, lr, e, \"Infield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(logs)\n",
    "# 2) Trains all Models and exports all data to an Excel Sheet\n",
    "max_depth = 50\n",
    "max_features = 30\n",
    "max_leaf_nodes = 150\n",
    "# could also add ways to change it for these hyperparams below for other models\n",
    "var_smoothing = 1e-9\n",
    "lr = 0.8\n",
    "e = 100\n",
    "rC = 1\n",
    "kernel='linear'\n",
    "degree= 1\n",
    "gamma= 'scale'\n",
    "coef0= 0.0\n",
    "\n",
    "runCount = int(config['TRAIN']['TimesRun'])\n",
    "if (\"False\" in config['TRAIN']['Testing']):\n",
    "     runCount = 1\n",
    "     print(\"Not Testing\")\n",
    "for j in range(runCount):\n",
    "        xTrain, xTest, yTrain, yTest = ModelUtil.modelDataSplitting(infieldDataFrame, j, 0.25,'InfieldTrainingFilter')\n",
    "        if(\"True\" in config['MODELS']['DTC']):\n",
    "            dtOutput = ModelUtil.runDT(xTrain, yTrain, xTest, yTest, max_depth, max_features, max_leaf_nodes)\n",
    "            if (\"True\" in config['DATA']['Pickle']):\n",
    "                # Save the model to a file\n",
    "                with open('Models/DecisionTree.pkl', 'wb') as file:\n",
    "                  pickle.dump(dtOutput, file)\n",
    "        if(\"True\" in config['MODELS']['NB']):   \n",
    "            nbOutput = ModelUtil.runNB(xTrain, yTrain, xTest, yTest, var_smoothing)\n",
    "            if (\"True\" in config['DATA']['Pickle']):\n",
    "                # Save the model to a file\n",
    "                with open('Models/NaiveBayes.pkl', 'wb') as file:\n",
    "                  pickle.dump(nbOutput, file)\n",
    "        if(\"True\" in config['MODELS']['LR']):\n",
    "            logRegOutput = ModelUtil.runLogReg(xTrain, yTrain, xTest, yTest, lr, e)\n",
    "            if (\"True\" in config['DATA']['Pickle']):\n",
    "                # Save the model to a file\n",
    "                with open('Models/LogRegression.pkl', 'wb') as file:\n",
    "                  pickle.dump(logRegOutput, file)\n",
    "        if(\"True\" in config['MODELS']['SVM']):\n",
    "            svmOutput = ModelUtil.runSVM(xTrain, yTrain, xTest, yTest, rC, kernel, degree, gamma, coef0)\n",
    "            if (\"True\" in config['DATA']['Pickle']):\n",
    "                # Save the model to a file\n",
    "                with open('Models/SVM.pkl', 'wb') as file:\n",
    "                  pickle.dump(svmOutput, file)\n",
    "        # if(\"True\" in config['MODELS']['RF']):\n",
    "        #     for i in range(0, len(trainIn)):\n",
    "        #         direction, distance = ModelUtil.runRFR(trainIn[i], trainOut[i], testIn[i], testOut[i])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ModelUtil)\n",
    "importlib.reload(logs)\n",
    "# a) Decision Tree\n",
    "# Need to test these hyperparameters for best case\n",
    "# Maybe make a way to superset these\n",
    "max_depth =      [50, 40]\n",
    "max_features =   [30, 20]\n",
    "max_leaf_nodes = [150, 100]\n",
    "hyperparamlist = []\n",
    "# This just makes the permutations of the hyperparameters above. Lets you test on many hyperparams.\n",
    "for n in range(len(max_depth)):\n",
    "    for k in range(len(max_features)):\n",
    "        for m in range(len(max_leaf_nodes)):\n",
    "            hyperparamlist.append([max_depth[n], max_features[k], max_leaf_nodes[m]])\n",
    "            \n",
    "# for each permutation, it runs a certain amount of time that you specify in the config (30 rn bc of Dozier) and saves the outcome to an excel sheet\n",
    "# requires to rerun the training set every time because otherwise will give you the same outcome every time\n",
    "# Also proves that its the models ability, not the luck of the draw for the data\n",
    "for lst in hyperparamlist:\n",
    "    runCount = int(config['TRAIN']['TimesRun'])\n",
    "    if (\"False\" in config['TRAIN']['Testing']):\n",
    "        runCount = 1\n",
    "        print(\"Not Testing\")\n",
    "    for j in range(runCount):\n",
    "        print(infieldDataFrame)\n",
    "        xTrain, xTest, yTrain, yTest = ModelUtil.modelDataSplitting(infieldDataFrame, j, 0.25,'InfieldTrainingFilter')\n",
    "        dtOutput = ModelUtil.runDT(xTrain, yTrain, xTest, yTest, lst[0], lst[1], lst[2])\n",
    "        if (\"True\" in config['DATA']['Pickle']):\n",
    "            # Save the model to a file\n",
    "            with open('Models/DecisionTree.pkl', 'wb') as file:\n",
    "                pickle.dump(dtOutput, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ModelUtil)\n",
    "importlib.reload(logs)\n",
    "# b) Naive Bayes\n",
    "\n",
    "var_smoothing = 1e-9\n",
    "runCount = int(config['TRAIN']['TimesRun'])\n",
    "if (\"False\" in config['TRAIN']['Testing']):\n",
    "     runCount = 1\n",
    "     print(\"Not Testing\")\n",
    "for j in range(runCount):\n",
    "        xTrain, xTest, yTrain, yTest = ModelUtil.modelDataSplitting(infieldDataFrame, j, 0.25,'InfieldTrainingFilter')\n",
    "        nbOutput = ModelUtil.runNB(xTrain, yTrain, xTest, yTest, var_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ModelUtil)\n",
    "importlib.reload(logs)\n",
    "# c)Logistic Regression\n",
    "lr = 0.8\n",
    "e = 100\n",
    "runCount = int(config['TRAIN']['TimesRun'])\n",
    "if (\"False\" in config['TRAIN']['Testing']):\n",
    "     runCount = 1\n",
    "     print(\"Not Testing\")\n",
    "for j in range(runCount):\n",
    "     xTrain, xTest, yTrain, yTest = ModelUtil.modelDataSplitting(infieldDataFrame, j, 0.25,'InfieldTrainingFilter')\n",
    "     logRegOutput = ModelUtil.runLogReg(xTrain, yTrain, xTest, yTest, lr, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ModelUtil)\n",
    "importlib.reload(logs)\n",
    "# d) SVM\n",
    "rC = 1\n",
    "kernel='linear'\n",
    "degree= 1\n",
    "gamma= 'scale'\n",
    "coef0= 0.0\n",
    "runCount = int(config['TRAIN']['TimesRun'])\n",
    "if (\"False\" in config['TRAIN']['Testing']):\n",
    "     runCount = 1\n",
    "     print(\"Not Testing\")\n",
    "for j in range(runCount):\n",
    "     xTrain, xTest, yTrain, yTest = ModelUtil.modelDataSplitting(infieldDataFrame, j, 0.25,'InfieldTrainingFilter')\n",
    "     svmOutput = ModelUtil.runSVM(xTrain, yTrain, xTest, yTest, rC, kernel, degree, gamma, coef0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # z) RandomForestRegressor\n",
    "# for i in range(0, len(trainIn)):\n",
    "#     direction, distance = ModelUtil.runRFR(trainIn[i], trainOut[i], testIn[i], testOut[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# This is meant to take all the values from the 30 runs and average them and output them to another sheet of averages for different models\n",
    "# Then will need to do this for all the models\n",
    "# Can take this and put it into an excelAverages function\n",
    "#prob rename this\n",
    "\n",
    "# could move these column letter names and do something with that so not hardcoded\n",
    "if(\"True\" in config['LOGGING']['Excel']):\n",
    "    sColumns = ['Training Accuracy', 'Testing Accuracy', 'Training Average Error', 'Testing Average Error', 'Training F1(micro)', 'Training F1(macro)', 'Training F1(weighted)', \n",
    "                'Testing F1(micro)', 'Testing F1(macro)', 'Testing F1(weighted)', 'Training AUC(macro)', 'Training AUC(weighted)', 'Testing AUC(macro)', 'Testing AUC(weighted)', \n",
    "                'Section 0 Probability', 'Section 1 Probability', 'Section 2 Probability', 'Section 3 Probability', 'Section 4 Probability']\n",
    "    if(\"True\" in config['MODELS']['DTC']):\n",
    "        # columns in excel: I J K L W X Y Z AA AB AC AD AE AF AG AH AI AJ AK   \n",
    "        sColumnsLetter = ['I','J','K','L','W','X','Y','Z','AA','AB','AC','AD','AE','AF','AG','AH','AI','AJ','AK']\n",
    "        logs.excelAverages('DecisionTree',sColumns,sColumnsLetter)\n",
    "    if(\"True\" in config['MODELS']['NB']):\n",
    "        sColumnsLetter = ['D','E','F','G','R','S','T','U','V','W','X','Y','Z','AA','AB','AC','AD','AE','AF']\n",
    "        logs.excelAverages('NaiveBayes',sColumns,sColumnsLetter)\n",
    "    if(\"True\" in config['MODELS']['LR']):\n",
    "        sColumnsLetter = ['E','F','G','H','S','T','U','V','W','X','Y','Z','AA','AB','AC','AD','AE','AF','AG']\n",
    "        logs.excelAverages('LogisticRegression',sColumns,sColumnsLetter)\n",
    "    if(\"True\" in config['MODELS']['SVM']):\n",
    "        sColumnsLetter = ['H','I','J','K','V','W','X','Y','Z','AA','AB','AC','AD','AE','AF','AG','AH','AI','AJ']\n",
    "        logs.excelAverages('SVM',sColumns,sColumnsLetter)\n",
    "    if(\"True\" in config['MODELS']['RF']):\n",
    "        logs.excelAverages('RandomForest',sColumns,sColumnsLetter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the value of index to look at different datapoints\n",
    "importlib.reload(VisualUtil)\n",
    "# 3) Model Testing:\n",
    "dt = dtOutput[0]\n",
    "nb = nbOutput[0]\n",
    "logReg = logRegOutput[0]\n",
    "# svm = svmOutput[0]\n",
    "\n",
    "print(\"Testing Output: \")\n",
    "# index of test value:\n",
    "index = 4555\n",
    "print(f\"Actual Field Slice: \\t\\t{yTest.iloc[index]}\")\n",
    "\n",
    "print(\"\\nDecision Tree:\")\n",
    "print(f\"Predicted Field Slice: \\t\\t{dt.predict([xTest.iloc[index]])[0]}\")\n",
    "print(f\"Field Slice Probabilities: \\t{dt.predict_proba([xTest.iloc[index]])[0]}\")\n",
    "\n",
    "print(\"\\nNaive Bayes:\")\n",
    "print(f\"Predicted Field Slice: \\t\\t{nb.predict([xTest.iloc[index]])[0]}\")\n",
    "print(f\"Field Slice Probabilities: \\t{nb.predict_proba([xTest.iloc[index]])[0]}\")\n",
    "\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print(f\"Predicted Field Slice: \\t\\t{logReg.predict([xTest.iloc[index]])[0]}\")\n",
    "print(f\"Field Slice Probabilities: \\t{logReg.predict_proba([xTest.iloc[index]])[0]}\")\n",
    "\n",
    "# print(\"\\nSVM:\")\n",
    "# print(f\"Predicted Field Slice: \\t\\t{svm.predict([xTest.iloc[index]])[0]}\")\n",
    "# print(f\"Field Slice Probabilities: \\t{svm.predict_proba([xTest.iloc[index]])[0]}\")\n",
    "\n",
    "averageProbs = dt.predict_proba([xTest.iloc[index]])[0] + nb.predict_proba([xTest.iloc[index]])[0] + logReg.predict_proba([xTest.iloc[index]])[0] # + svm.predict_proba([xTest.iloc[index]])[0]\n",
    "averageProbs = averageProbs / 3 \n",
    "\n",
    "print(f\"\\n\\nAVG Prediction: \\t\\t{np.argmax(averageProbs)+1}\")\n",
    "print(f\"Field Slice AVG Probabilities: \\t{averageProbs}\")\n",
    "\n",
    "VisualUtil.visualizeData(averageProbs, [1], 'TestPic.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Data Visualization\n",
    "importlib.reload(VisualUtil)\n",
    "\n",
    "# Temporary method of getting percentages for testing purposes\n",
    "infieldPercentages  = np.random.dirichlet(np.ones(5), size=1)[0]\n",
    "outfieldPercentages = np.random.dirichlet(np.ones(2), size=1)[0]\n",
    "outfieldCoordinates = np.random.uniform(low=[-45, 150], high=[45, 400], size=(30,2))\n",
    "\n",
    "outfieldCoordinates = [0,1,0,0,3,0,0,0,0,0,0,0,0,5,0]\n",
    "VisualUtil.visualizeData(infieldPercentages, outfieldCoordinates, \"FieldTest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [0,1,2,3]\n",
    "o = [2,3,4,5]\n",
    "\n",
    "print(l+o)\n",
    "l.append(o)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Pitcher Data Processing and Running\n",
    "importlib.reload(Preprocessing)\n",
    "importlib.reload(DataUtil)\n",
    "importlib.reload(VisualUtil)\n",
    "importlib.reload(batch_image_to_excel)\n",
    "importlib.reload(logs)\n",
    "\n",
    "\n",
    "pitchingAveragesDF = DataUtil.getRawDataFrame('Data/PitchMetricAverages_AsOf_2024-03-11.csv', [])\n",
    "# drop nan values from the used columns\n",
    "specific_columns = [\"PitcherThrows\", \"BatterSide\", \"TaggedPitchType\", \"RelSpeed\", \"InducedVertBreak\", \"HorzBreak\", \"RelHeight\", \"RelSide\", \"SpinAxis\", \"SpinRate\", \"VertApprAngle\", \"HorzApprAngle\"] # pitcher averages\n",
    "infieldDataFrame = infieldDataFrame[specific_columns] \n",
    "outfieldDataFrame = outfieldDataFrame[specific_columns]\n",
    "averagesX = pitchingAveragesDF[specific_columns] # pitcher averages\n",
    "#averagesX = averagesX.dropna(axis=0, how='any',subset=specific_columns)\n",
    "#averagesX = averagesX[[\"PitcherThrows\", \"BatterSide\", \"TaggedPitchType\", \"PlateLocHeight\", \"PlateLocSide\", \"ZoneSpeed\", \"RelSpeed\", \"SpinRate\", \"HorzBreak\", \"VertBreak\"]]\n",
    "\n",
    "averagesX[\"PitcherThrows\"] = averagesX[\"PitcherThrows\"].map({\"Left\":1, \"Right\":2, \"Both\":3})\n",
    "averagesX[\"BatterSide\"] = averagesX[\"BatterSide\"].map({\"Left\":1, \"Right\":2})\n",
    "averagesX[\"TaggedPitchType\"] = averagesX[\"TaggedPitchType\"].map({\"Fastball\": 1, \"FourSeamFastBall\":1, \"Sinker\":2, \"TwoSeamFastBall\":2, \"Cutter\":3, \"Curveball\":4, \"Slider\":5, \"ChangeUp\":6, \"Splitter\":7, \"Knuckleball\":8})\n",
    "\n",
    "# normalize this based on min and maxes from training data\n",
    "averagesX = DataUtil.normalizeData(averagesX, infieldDataFrame)\n",
    "\n",
    "# Change the value of index to look at different datapoints\n",
    "importlib.reload(VisualUtil)\n",
    "# 3) Model Testing:\n",
    "dt = dtOutput[0]\n",
    "nb = nbOutput[0]\n",
    "logReg = logRegOutput[0]\n",
    "dto = OutfielddtOutput[0]\n",
    "nbo = OutfieldnbOutput[0]\n",
    "logRego = OutfieldlogRegOutput[0]\n",
    "# svm = svmOutput[0]\n",
    "for index in range(pitchingAveragesDF.shape[0]):\n",
    "    #print(index)\n",
    "    averageProbs= []\n",
    "    averageProbs = dt.predict_proba([averagesX.iloc[index]])[0] + nb.predict_proba([averagesX.iloc[index]])[0] + logReg.predict_proba([averagesX.iloc[index]])[0]\n",
    "    averageProbs = averageProbs / 3 \n",
    "\n",
    "    averageProbso= []\n",
    "    averageProbso = dto.predict_proba([averagesX.iloc[index]])[0] + nbo.predict_proba([averagesX.iloc[index]])[0] + logRego.predict_proba([averagesX.iloc[index]])[0]\n",
    "    averageProbso = averageProbso / 3 \n",
    "\n",
    "    # print(f\"\\n\\nAVG Prediction: \\t\\t{np.argmax(averageProbs)+1}\")\n",
    "    # print(f\"Field Slice AVG Probabilities: \\t{averageProbs}\")\n",
    "    fileName = pitchingAveragesDF.iloc[index][0].replace(\",\", \"_\").replace(\" \", \"\") + \"_\" + pitchingAveragesDF.iloc[index][\"TaggedPitchType\"] + \"_\" + pitchingAveragesDF.iloc[index][\"BatterSide\"] + \"Batter\"\n",
    "    VisualUtil.visualizeData(averageProbs, averageProbso, fileName)   \n",
    "\n",
    "batch_image_to_excel.create_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Pitcher Data Processing and Running\n",
    "importlib.reload(Preprocessing)\n",
    "importlib.reload(DataUtil)\n",
    "importlib.reload(VisualUtil)\n",
    "importlib.reload(batch_image_to_excel)\n",
    "import math\n",
    "\n",
    "\n",
    "pitchingAveragesDF = DataUtil.getRawDataFrame('Data/PitchMetricAverages_AsOf_2024-03-11.csv', [])\n",
    "# drop nan values from the used columns\n",
    "specific_columns = [\"PitcherThrows\", \"BatterSide\", \"TaggedPitchType\", \"RelSpeed\", \"InducedVertBreak\", \"HorzBreak\", \"RelHeight\", \"RelSide\", \"SpinAxis\", \"SpinRate\", \"VertApprAngle\", \"HorzApprAngle\"]#, \"Extension\"] # pitcher averages\n",
    "outfieldDataFrame = outfieldDataFrame[specific_columns] \n",
    "infieldDataFrame = infieldDataFrame[specific_columns]\n",
    "averagesX = pitchingAveragesDF[specific_columns] # pitcher averages\n",
    "#averagesX = averagesX[[\"PitcherThrows\", \"BatterSide\", \"TaggedPitchType\", \"PlateLocHeight\", \"PlateLocSide\", \"ZoneSpeed\", \"RelSpeed\", \"SpinRate\", \"HorzBreak\", \"VertBreak\"]]\n",
    "averagesX[\"PitcherThrows\"] = averagesX[\"PitcherThrows\"].map({\"Left\":1, \"Right\":2, \"Both\":3})\n",
    "averagesX[\"BatterSide\"] = averagesX[\"BatterSide\"].map({\"Left\":1, \"Right\":2})\n",
    "averagesX[\"TaggedPitchType\"] = averagesX[\"TaggedPitchType\"].map({\"Fastball\": 1, \"FourSeamFastBall\":1, \"Sinker\":2, \"TwoSeamFastBall\":2, \"Cutter\":3, \"Curveball\":4, \"Slider\":5, \"ChangeUp\":6, \"Splitter\":7, \"Knuckleball\":8})\n",
    "\n",
    "# normalize this based on min and maxes from training data\n",
    "averagesX = DataUtil.normalizeData(averagesX, infieldDataFrame)\n",
    "\n",
    "# Change the value of index to look at different datapoints\n",
    "importlib.reload(VisualUtil)\n",
    "# 3) Model Testing:\n",
    "dt = dtOutput[0]\n",
    "nb = nbOutput[0]\n",
    "logReg = logRegOutput[0]\n",
    "\n",
    "dto = OutfielddtOutput[0]\n",
    "nbo = OutfieldnbOutput[0]\n",
    "logRego = OutfieldlogRegOutput[0]\n",
    "# svm = svmOutput[0]\n",
    "for index in range(pitchingAveragesDF.shape[0]):\n",
    "    #print(index)\n",
    "    averageProbs= []\n",
    "    averageProbs = dt.predict_proba([averagesX.iloc[index]])[0] + nb.predict_proba([averagesX.iloc[index]])[0] + logReg.predict_proba([averagesX.iloc[index]])[0]\n",
    "    averageProbs = averageProbs / 3 \n",
    "\n",
    "    averageProbso= []\n",
    "    averageProbso = dto.predict_proba([averagesX.iloc[index]])[0] + nbo.predict_proba([averagesX.iloc[index]])[0] + logRego.predict_proba([averagesX.iloc[index]])[0]\n",
    "    averageProbso = averageProbso / 3 \n",
    "\n",
    "    # print(f\"\\n\\nAVG Prediction: \\t\\t{np.argmax(averageProbs)+1}\")\n",
    "    # print(f\"Field Slice AVG Probabilities: \\t{averageProbs}\")\n",
    "    fileName = pitchingAveragesDF.iloc[index][0].replace(\",\", \"_\").replace(\" \", \"\") + \"_\" + pitchingAveragesDF.iloc[index][\"TaggedPitchType\"] + \"_\" + pitchingAveragesDF.iloc[index][\"BatterSide\"] + \"Batter\"\n",
    "    VisualUtil.visualizeData(averageProbs, averageProbso, fileName)   \n",
    "\n",
    "batch_image_to_excel.create_excel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
