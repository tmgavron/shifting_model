{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training:\n",
    "# 1) Load all data from preprocessing (training/test splits, etc)\n",
    "# 2) Begin Training Models\n",
    "    #  a) Decision Tree\n",
    "    #  b) Naive Bayes\n",
    "    #  c) Logistic Regression\n",
    "    #  d) SVM\n",
    "# 3) Testing Models\n",
    "# 4) New Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from Models import ModelUtil\n",
    "from Data import Preprocessing, DataUtil\n",
    "from Visualization import VisualUtil\n",
    "from Logs import logging as logs\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import importlib\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.model_selection import KFold\n",
    "#from sklearn.model_selection import LeaveOneOut\n",
    "import configparser\n",
    "import numpy as np\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('Data//config.ini')\n",
    "\n",
    "importlib.reload(Preprocessing)\n",
    "importlib.reload(ModelUtil)\n",
    "importlib.reload(VisualUtil)\n",
    "importlib.reload(logs)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load all data from preprocessing \n",
    "importlib.reload(Preprocessing)\n",
    "newprocessing = 'True' in config['DATA']['USE_NEW_PREPROCESSING']\n",
    "infieldDataFrame, outfieldDataFrame = Preprocessing.dataFiltering([], newprocessing)\n",
    "# do we need to add Bearing to outfield?\n",
    "\n",
    "# Moved to filtering methods in Datautil used by dataFiltering method in Preprocessing\n",
    "# The specific columns can be used in the config now. These ones here would be put under the InfieldOverallFiltering \n",
    "\n",
    "\n",
    "# if(\"False\" in config['DATA']['USE_NEW_PREPROCESSING']):\n",
    "#     infieldDataFrame, outfieldDataFrame = Preprocessing.dataFiltering([], False)\n",
    "\n",
    "# drop nan values from the used columns\n",
    "#     specific_columns = [\"PitcherThrows\", \"BatterSide\", \"TaggedPitchType\", \"PlateLocHeight\", \"PlateLocSide\", \"ZoneSpeed\", \"SpinRate\", \"RelSpeed\", \"HorzBreak\", \"VertBreak\"]\n",
    "#     specific_columns = [\"PitcherThrows\", \"BatterSide\", \"TaggedPitchType\", \"RelSpeed\", \"VertRelAngle\", \"HorzRelAngle\", \"SpinRate\", \"SpinAxis\", \"RelHeight\", \"RelSide\", \"VertBreak\", \"InducedVertBreak\", \"HorzBreak\", \"VertApprAngle\", \"HorzApprAngle\"] # pitcher averages\n",
    "#     specific_columns = [\"PitcherThrows\", \"BatterSide\", \"TaggedPitchType\", \"RelSpeed\", \"InducedVertBreak\", \"HorzBreak\", \"RelHeight\", \"RelSide\", \"SpinAxis\", \"SpinRate\", \"VertApprAngle\", \"HorzApprAngle\", \"Extension\"] \n",
    "#     infieldDataFrame = infieldDataFrame.dropna(axis=0, how='any',subset=specific_columns)\n",
    "\n",
    "# else:\n",
    "#     normDataFrame = Preprocessing.dataProcessing()\n",
    "#     infieldDataFrame, outfieldDataFrame = Preprocessing.dataFiltering(normDataFrame, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of this is mapping the strings to numbers for both infieldDataFrame and outfieldDataFrame so that the correlation matrix can be computed\n",
    "# This can most likely be moved to a method in the logging.py file\n",
    "infieldDF4Matrix = infieldDataFrame.copy()\n",
    "outfieldDF4Matrix = outfieldDataFrame.copy()\n",
    "strColumns = [] \n",
    "for cName in outfieldDF4Matrix.columns:\n",
    "    if(str(outfieldDF4Matrix[cName].dtype) in 'object'):\n",
    "        strColumns.append(cName)\n",
    "rValueDict = {}\n",
    "for cName in strColumns:\n",
    "    i = 0\n",
    "    infieldUniques = infieldDF4Matrix[cName].unique()\n",
    "    for rValue in infieldUniques:\n",
    "        rValueDict.update({rValue:i})\n",
    "        i+=1\n",
    "    infieldDF4Matrix[cName] = infieldDF4Matrix[cName].map(rValueDict)\n",
    "    uniqueVals = [x for x in outfieldDF4Matrix[cName].unique() if x not in infieldUniques]\n",
    "    for rValue in uniqueVals: \n",
    "        rValueDict.update({rValue:i})\n",
    "        i+=1\n",
    "    outfieldDF4Matrix[cName] = outfieldDF4Matrix[cName].map(rValueDict)\n",
    "infieldDF4Matrix = infieldDF4Matrix.replace(np.nan, 0)\n",
    "infieldDF4Matrix = infieldDF4Matrix.replace('', 0)\n",
    "outfieldDF4Matrix = outfieldDF4Matrix.replace(np.nan, 0)\n",
    "outfieldDF4Matrix = outfieldDF4Matrix.replace('', 0)\n",
    "\n",
    "# Correlation does not imply causation.\n",
    "# -1 means that the 2 variables have an inverse linear relationship: when X increases, Y decreases\n",
    "# 0 means no linear correlation between X and Y\n",
    "# 1 means that the 2 variables have a linear relationship: when X increases, Y increases too.\n",
    "infieldcorrmatrix = infieldDF4Matrix.corr()\n",
    "outfieldcorrmatrix = outfieldDF4Matrix.corr()\n",
    "if (config['LOGGING']['Excel'] == 'True'):\n",
    "    logs.writeToExcelSheet(infieldcorrmatrix, \"Infield Correlation Matrix\")\n",
    "    logs.writeToExcelSheet(outfieldcorrmatrix, \"Outfield Correlation Matrix\")\n",
    "if (config['LOGGING']['Debug'] == 'True'):\n",
    "    print(infieldcorrmatrix)\n",
    "    print(outfieldcorrmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''importlib.reload(Preprocessing)\n",
    "importlib.reload(DataUtil)\n",
    "\n",
    "if(\"False\" in config['DATA']['USE_NEW_PREPROCESSING']):\n",
    "    Y = infieldDataFrame[\"FieldSlice\"]\n",
    "    X = infieldDataFrame[[\"PitcherThrows\", \"BatterSide\", \"TaggedPitchType\", \"PlateLocHeight\", \"PlateLocSide\", \"ZoneSpeed\", \"SpinRate\", \"RelSpeed\", \"HorzBreak\", \"VertBreak\"]] \n",
    "    X = infieldDataFrame[specific_columns] # pitcher averages\n",
    "    originalNotNormX = X\n",
    "    X = DataUtil.normalizeData(X, originalNotNormX)\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size=0.25, random_state=11)\n",
    "    # adb = AdaBoostClassifier()\n",
    "    # adb_model = adb.fit(xTrain, yTrain)\n",
    "\n",
    "    # calculate split information:\n",
    "    trainingClassSplit = [0, 0, 0, 0, 0]\n",
    "    for i in yTrain:\n",
    "        trainingClassSplit[i-1] += 1\n",
    "\n",
    "    \n",
    "    testingClassSplit = [0, 0, 0, 0, 0]\n",
    "    for i in yTest:\n",
    "        testingClassSplit[i-1] += 1\n",
    "\n",
    "    trainingClassPercent = []\n",
    "    for i in trainingClassSplit:\n",
    "        trainingClassPercent.append(round(i/len(yTrain), 4))\n",
    "\n",
    "    testingClassPercent = []\n",
    "    for i in testingClassSplit:\n",
    "        testingClassPercent.append(round(i/len(yTest), 4))\n",
    "\n",
    "    print(\"Training Class Splits (count, then percentage):\")\n",
    "    print(trainingClassSplit)\n",
    "    print(trainingClassPercent)\n",
    "    print(\"\\nTesting Class Splits (count, then percentage):\")\n",
    "    print(testingClassSplit)\n",
    "    print(testingClassPercent)\n",
    "else:\n",
    "    infieldY = infieldDataFrame[0][['Direction','Distance']]\n",
    "    infieldX = infieldDataFrame[0][infieldDataFrame[1]] \n",
    "    if(\"True\" in config['SPLIT']['TTS']):\n",
    "        xTrain, xTest, yTrain, yTest = train_test_split(infieldX, infieldY, test_size=0.20, random_state=11)\n",
    "        \n",
    "    elif(\"True\" in config['SPLIT']['KFold']):\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=11)\n",
    "        for train_index, test_index in kf.split(infieldX):\n",
    "            xTrain, xTest = infieldX.iloc[train_index,:], infieldX.iloc[test_index,:]\n",
    "            yTrain, yTest = infieldY.iloc[train_index,:], infieldY.iloc[test_index,:]\n",
    "\n",
    "    elif(\"True\" in config['SPLIT']['LOOCV']):\n",
    "        loo = LeaveOneOut()\n",
    "        for train_index, test_index in loo.split(infieldX):\n",
    "            xTrain, xTest = infieldX.iloc[train_index,:], infieldX.iloc[test_index,:]\n",
    "            yTrain, yTest = infieldY.iloc[train_index,:], infieldY.iloc[test_index,:]\n",
    "\n",
    "    else:\n",
    "        print(\"No Splitting Method Selected\")\n",
    "        \n",
    "\n",
    "# GroupKFold: (avoids putting data from the same group in the test set -- useful for Pitcher/Batter ID when we implement that.)'''#AdaBoostClassifier\n",
    "\n",
    "# Has all been moved to this function below which is used in the code block below because we need to redo the training splits every time we run it bc otherwise\n",
    "#   we get the same output every time\n",
    "# ModelUtil.modelDataSplitting(dF, randomState, dFType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Trains all Models and exports all data to an Excel Sheet\n",
    "max_depth = 50\n",
    "max_features = 30\n",
    "max_leaf_nodes = 150\n",
    "# could also add ways to change it for these hyperparams below for other models\n",
    "var_smoothing = 1e-9\n",
    "lr = 0.8\n",
    "e = 100\n",
    "rC = 1\n",
    "kernel='linear'\n",
    "degree= 1\n",
    "gamma= 'scale'\n",
    "coef0= 0.0\n",
    "\n",
    "for j in range(int(config['TRAIN']['TimesRun'])):\n",
    "        xTrain, xTest, yTrain, yTest = ModelUtil.modelDataSplitting(infieldDataFrame, j, 0.25,'InfieldTrainingFilter')\n",
    "        if(\"True\" in config['MODELS']['DTC']):\n",
    "            dtOutput = ModelUtil.runDT(xTrain, yTrain, xTest, yTest, max_depth, max_features, max_leaf_nodes)\n",
    "        if(\"True\" in config['MODELS']['NB']):   \n",
    "            nbOutput = ModelUtil.runNB(xTrain, yTrain, xTest, yTest, var_smoothing)\n",
    "        if(\"True\" in config['MODELS']['LR']):\n",
    "            logRegOutput = ModelUtil.runLogReg(xTrain, yTrain, xTest, yTest, lr, e)\n",
    "        if(\"True\" in config['MODELS']['SVM']):\n",
    "            svmOutput = ModelUtil.runSVM(xTrain, yTrain, xTest, yTest, rC, kernel, degree, gamma, coef0)\n",
    "        if(\"True\" in config['MODELS']['RF']):\n",
    "            for i in range(0, len(trainIn)):\n",
    "                direction, distance = ModelUtil.runRFR(trainIn[i], trainOut[i], testIn[i], testOut[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ModelUtil)\n",
    "importlib.reload(logs)\n",
    "# a) Decision Tree\n",
    "# Need to test these hyperparameters for best case\n",
    "# Maybe make a way to superset these\n",
    "max_depth =      [50, 40]\n",
    "max_features =   [30, 20]\n",
    "max_leaf_nodes = [150, 100]\n",
    "hyperparamlist = []\n",
    "# This just makes the permutations of the hyperparameters above. Lets you test on many hyperparams.\n",
    "for n in range(len(max_depth)):\n",
    "    for k in range(len(max_features)):\n",
    "        for m in range(len(max_leaf_nodes)):\n",
    "            hyperparamlist.append([max_depth[n], max_features[k], max_leaf_nodes[m]])\n",
    "            \n",
    "# for each permutation, it runs a certain amount of time that you specify in the config (30 rn bc of Dozier) and saves the outcome to an excel sheet\n",
    "# requires to rerun the training set every time because otherwise will give you the same outcome every time\n",
    "# Also proves that its the models ability, not the luck of the draw for the data\n",
    "for lst in hyperparamlist:\n",
    "    for j in range(int(config['TRAIN']['TimesRun'])):\n",
    "        xTrain, xTest, yTrain, yTest = ModelUtil.modelDataSplitting(infieldDataFrame, j, 0.25,'InfieldTrainingFilter')\n",
    "        dtOutput = ModelUtil.runDT(xTrain, yTrain, xTest, yTest, lst[0], lst[1], lst[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# This is meant to take all the values from the 30 runs and average them and output them to another sheet of averages for different models\n",
    "# Then will need to do this for all the models\n",
    "# Can take this and put it into an excelAverages function\n",
    "#prob rename this\n",
    "\n",
    "# could move these column letter names and do something with that so not hardcoded\n",
    "if(\"True\" in config['LOGGING']['Excel']):\n",
    "    sColumns = ['Training Accuracy', 'Testing Accuracy', 'Training Average Error', 'Testing Average Error', 'Training F1(micro)', 'Training F1(macro)', 'Training F1(weighted)', \n",
    "                'Testing F1(micro)', 'Testing F1(macro)', 'Testing F1(weighted)', 'Training AUC(macro)', 'Training AUC(weighted)', 'Testing AUC(macro)', 'Testing AUC(weighted)', \n",
    "                'Section 0 Probability', 'Section 1 Probability', 'Section 2 Probability', 'Section 3 Probability', 'Section 4 Probability']\n",
    "    if(\"True\" in config['MODELS']['DTC']):\n",
    "        # columns in excel: I J K L W X Y Z AA AB AC AD AE AF AG AH AI AJ AK   \n",
    "        sColumnsLetter = ['I','J','K','L','W','X','Y','Z','AA','AB','AC','AD','AE','AF','AG','AH','AI','AJ','AK']\n",
    "        logs.excelAverages('DecisionTree',sColumns,sColumnsLetter)\n",
    "    if(\"True\" in config['MODELS']['NB']):\n",
    "        sColumnsLetter = ['D','E','F','G','R','S','T','U','V','W','X','Y','Z','AA','AB','AC','AD','AE','AF']\n",
    "        logs.excelAverages('NaiveBayes',sColumns,sColumnsLetter)\n",
    "    if(\"True\" in config['MODELS']['LR']):\n",
    "        sColumnsLetter = ['E','F','G','H','S','T','U','V','W','X','Y','Z','AA','AB','AC','AD','AE','AF','AG']\n",
    "        logs.excelAverages('LogisticRegression',sColumns,sColumnsLetter)\n",
    "    if(\"True\" in config['MODELS']['SVM']):\n",
    "        sColumnsLetter = ['H','I','J','K','V','W','X','Y','Z','AA','AB','AC','AD','AE','AF','AG','AH','AI','AJ']\n",
    "        logs.excelAverages('SVM',sColumns,sColumnsLetter)\n",
    "    if(\"True\" in config['MODELS']['RF']):\n",
    "        logs.excelAverages('RandomForest',sColumns,sColumnsLetter)\n",
    "\n",
    "# Moved all this code to excelAverages to be able to repeat it\n",
    "\n",
    "# from datetime import datetime\n",
    "# import openpyxl\n",
    "\n",
    "# now = datetime.now()\n",
    "# dt_string = now.strftime(\"_%m-%Y\")\n",
    "# filename = f\"logs/ModelStatistics{dt_string}.xlsx\"\n",
    "# wb = openpyxl.load_workbook(filename)\n",
    "# #first_sheet = wb.get_sheet_names()[0]\n",
    "# worksheet = wb.get_sheet_by_name('DecisionTree')\n",
    "# # These are the columns from the excel sheet that we want to average and put on Averages page\n",
    "# sColumns = ['Training Accuracy', 'Testing Accuracy', 'Training Average Error', 'Testing Average Error', 'Training F1(micro)', 'Training F1(macro)', 'Training F1(weighted)', 'Testing F1(micro)', 'Testing F1(macro)', 'Testing F1(weighted)', 'Training AUC(macro)', 'Training AUC(weighted)', 'Testing AUC(macro)', 'Testing AUC(weighted)', 'Section 0 Probability', 'Section 1 Probability', 'Section 2 Probability', 'Section 3 Probability', 'Section 4 Probability']\n",
    "# #           I J K L W X Y Z AA AB AC AD AE AF AG AH AI AJ AK   \n",
    "# sColumnsLetter = ['I','J','K','L','W','X','Y','Z','AA','AB','AC','AD','AE','AF','AG','AH','AI','AJ','AK']\n",
    "\n",
    "# # here you iterate over the rows in the specific column\n",
    "# # could also add the averages of just the certain permutation of hyperparams to see how its doing\n",
    "# # So an overall average and then\n",
    "# avgList = [0] * len(sColumns)\n",
    "# for row in range(2,worksheet.max_row+1): \n",
    "#     num = 0 \n",
    "#     for i, column in enumerate(sColumnsLetter):  #Here you can add or reduce the columns\n",
    "#         cell_name = \"{}{}\".format(column, row)\n",
    "#         avgList[i] =+ worksheet[cell_name].value # the value of the specific cell\n",
    "        \n",
    "#     num += 1\n",
    "    \n",
    "# avgList = [x/num for x in avgList]\n",
    "# avgList.insert(0, 'DecisionTree')\n",
    "# sColumns.insert(0, 'Model Type')\n",
    "# # Still needs to export to the workbooks like everything else\n",
    "# if (config['LOGGING']['Debug'] == 'True'):\n",
    "#     print(\"printing statistics...\")\n",
    "#     print(avgList)\n",
    "# if (config['LOGGING']['Excel'] == 'True'):\n",
    "#     print(\"exporting statistics to Excel...\")\n",
    "#     df = pd.DataFrame([avgList], columns=sColumns)\n",
    "#     logs.writeToExcelSheet(df,\"Average Statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ModelUtil)\n",
    "importlib.reload(logs)\n",
    "# b) Naive Bayes\n",
    "\n",
    "var_smoothing = 1e-9\n",
    "for j in range(int(config['TRAIN']['TimesRun'])):\n",
    "        xTrain, xTest, yTrain, yTest = ModelUtil.modelDataSplitting(infieldDataFrame, j, 0.25,'InfieldTrainingFilter')\n",
    "        nbOutput = ModelUtil.runNB(xTrain, yTrain, xTest, yTest, var_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ModelUtil)\n",
    "importlib.reload(logs)\n",
    "# c)Logistic Regression\n",
    "lr = 0.8\n",
    "e = 100\n",
    "logRegOutput = ModelUtil.runLogReg(xTrain, yTrain, xTest, yTest, lr, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ModelUtil)\n",
    "importlib.reload(logs)\n",
    "# d) SVM\n",
    "rC = 1\n",
    "kernel='linear'\n",
    "degree= 1\n",
    "gamma= 'scale'\n",
    "coef0= 0.0\n",
    "svmOutput = ModelUtil.runSVM(xTrain, yTrain, xTest, yTest, rC, kernel, degree, gamma, coef0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z) RandomForestRegressor\n",
    "for i in range(0, len(trainIn)):\n",
    "    direction, distance = ModelUtil.runRFR(trainIn[i], trainOut[i], testIn[i], testOut[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the value of index to look at different datapoints\n",
    "importlib.reload(VisualUtil)\n",
    "# 3) Model Testing:\n",
    "dt = dtOutput[0]\n",
    "nb = nbOutput[0]\n",
    "logReg = logRegOutput[0]\n",
    "# svm = svmOutput[0]\n",
    "\n",
    "print(\"Testing Output: \")\n",
    "# index of test value:\n",
    "index = 4555\n",
    "print(f\"Actual Field Slice: \\t\\t{yTest.iloc[index]}\")\n",
    "\n",
    "print(\"\\nDecision Tree:\")\n",
    "print(f\"Predicted Field Slice: \\t\\t{dt.predict([xTest.iloc[index]])[0]}\")\n",
    "print(f\"Field Slice Probabilities: \\t{dt.predict_proba([xTest.iloc[index]])[0]}\")\n",
    "\n",
    "print(\"\\nNaive Bayes:\")\n",
    "print(f\"Predicted Field Slice: \\t\\t{nb.predict([xTest.iloc[index]])[0]}\")\n",
    "print(f\"Field Slice Probabilities: \\t{nb.predict_proba([xTest.iloc[index]])[0]}\")\n",
    "\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print(f\"Predicted Field Slice: \\t\\t{logReg.predict([xTest.iloc[index]])[0]}\")\n",
    "print(f\"Field Slice Probabilities: \\t{logReg.predict_proba([xTest.iloc[index]])[0]}\")\n",
    "\n",
    "# print(\"\\nSVM:\")\n",
    "# print(f\"Predicted Field Slice: \\t\\t{svm.predict([xTest.iloc[index]])[0]}\")\n",
    "# print(f\"Field Slice Probabilities: \\t{svm.predict_proba([xTest.iloc[index]])[0]}\")\n",
    "\n",
    "averageProbs = dt.predict_proba([xTest.iloc[index]])[0] + nb.predict_proba([xTest.iloc[index]])[0] + logReg.predict_proba([xTest.iloc[index]])[0] # + svm.predict_proba([xTest.iloc[index]])[0]\n",
    "averageProbs = averageProbs / 3 \n",
    "\n",
    "print(f\"\\n\\nAVG Prediction: \\t\\t{np.argmax(averageProbs)+1}\")\n",
    "print(f\"Field Slice AVG Probabilities: \\t{averageProbs}\")\n",
    "\n",
    "VisualUtil.visualizeData(averageProbs, [1], 'TestPic.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather data on average predictions\n",
    "length = len(xTest)\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "wrongProbs = 0\n",
    "wrongDistance = 0\n",
    "\n",
    "dt = dtOutput[0]\n",
    "nb = nbOutput[0]\n",
    "logReg = logRegOutput[0]\n",
    "# svm = svmOutput[0]\n",
    "for index in range(length):\n",
    "    averageProbs = dt.predict_proba([xTest.iloc[index]])[0] + nb.predict_proba([xTest.iloc[index]])[0] + logReg.predict_proba([xTest.iloc[index]])[0] #+ svm.predict_proba([xTest.iloc[index]])[0]\n",
    "    averageProbs = averageProbs / 3\n",
    "\n",
    "    actual = yTest.iloc[index]\n",
    "    predicted = np.argmax(averageProbs)+1\n",
    "\n",
    "    percentageActual = averageProbs[actual-1]\n",
    "\n",
    "    if predicted == actual:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1 \n",
    "        wrongProbs += percentageActual\n",
    "        wrongDistance += abs(actual-predicted)\n",
    "\n",
    "\n",
    "# Correct Prediction Count\n",
    "print(correct)\n",
    "# Incorrect Prediction Count\n",
    "print(incorrect)\n",
    "# Average probability of actual slice when guess is incorrect\n",
    "print(wrongProbs/incorrect)\n",
    "# Average distance from actual slice to guess slice when guess is incorrect\n",
    "print(wrongDistance/incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Model Iterations and Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Data Visualization\n",
    "importlib.reload(VisualUtil)\n",
    "\n",
    "# Temporary method of getting percentages for testing purposes\n",
    "infieldPercentages  = np.random.dirichlet(np.ones(4), size=1)[0]\n",
    "outfieldPercentages = np.random.dirichlet(np.ones(2), size=1)[0]\n",
    "outfieldCoordinates = np.random.uniform(low=[-45, 150], high=[45, 400], size=(30,2))\n",
    "\n",
    "VisualUtil.visualizeData(infieldPercentages, outfieldCoordinates, \"FieldTest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Pitcher Data Processing and Running\n",
    "importlib.reload(Preprocessing)\n",
    "importlib.reload(DataUtil)\n",
    "\n",
    "pitchingAveragesDF = DataUtil.getRawDataFrame('Data/PitchMetricAverages_AsOf_2024-03-11.csv')\n",
    "# drop nan values from the used columns\n",
    "specific_columns = [\"PitcherThrows\", \"BatterSide\", \"TaggedPitchType\", \"RelSpeed\", \"InducedVertBreak\", \"HorzBreak\", \"RelHeight\", \"RelSide\", \"SpinAxis\", \"SpinRate\", \"VertApprAngle\", \"HorzApprAngle\"] # pitcher averages\n",
    "infieldDataFrame = infieldDataFrame[[\"PitcherThrows\", \"BatterSide\", \"TaggedPitchType\", \"PlateLocHeight\", \"PlateLocSide\", \"ZoneSpeed\", \"SpinRate\", \"RelSpeed\", \"HorzBreak\", \"VertBreak\"]] \n",
    "averagesX = pitchingAveragesDF[specific_columns] # pitcher averages\n",
    "#averagesX = averagesX[[\"PitcherThrows\", \"BatterSide\", \"TaggedPitchType\", \"PlateLocHeight\", \"PlateLocSide\", \"ZoneSpeed\", \"RelSpeed\", \"SpinRate\", \"HorzBreak\", \"VertBreak\"]]\n",
    "\n",
    "averagesX[\"PitcherThrows\"] = averagesX[\"PitcherThrows\"].map({\"Left\":1, \"Right\":2, \"Both\":3})\n",
    "averagesX[\"BatterSide\"] = averagesX[\"BatterSide\"].map({\"Left\":1, \"Right\":2})\n",
    "averagesX[\"TaggedPitchType\"] = averagesX[\"TaggedPitchType\"].map({\"Fastball\": 1, \"FourSeamFastBall\":1, \"Sinker\":2, \"TwoSeamFastBall\":2, \"Cutter\":3, \"Curveball\":4, \"Slider\":5, \"ChangeUp\":6, \"Splitter\":7, \"Knuckleball\":8})\n",
    "\n",
    "# normalize this based on min and maxes from training data\n",
    "averagesX = DataUtil.normalizeData(averagesX, infieldDataFrame)\n",
    "\n",
    "# Change the value of index to look at different datapoints\n",
    "importlib.reload(VisualUtil)\n",
    "# 3) Model Testing:\n",
    "dt = dtOutput[0]\n",
    "nb = nbOutput[0]\n",
    "logReg = logRegOutput[0]\n",
    "# svm = svmOutput[0]\n",
    "print(pitchingAveragesDF.iloc[11])\n",
    "for index in range(pitchingAveragesDF.shape[0]):\n",
    "    print(index)\n",
    "    averageProbs= []\n",
    "    averageProbs = dt.predict_proba([averagesX.iloc[index]])[0] + nb.predict_proba([averagesX.iloc[index]])[0] + logReg.predict_proba([averagesX.iloc[index]])[0]\n",
    "    averageProbs = averageProbs / 3 \n",
    "\n",
    "    # print(f\"\\n\\nAVG Prediction: \\t\\t{np.argmax(averageProbs)+1}\")\n",
    "    # print(f\"Field Slice AVG Probabilities: \\t{averageProbs}\")\n",
    "    fileName = pitchingAveragesDF.iloc[index][0].replace(\",\", \"_\").replace(\" \", \"\") + \"_\" + pitchingAveragesDF.iloc[index][\"TaggedPitchType\"] + \"_\" + pitchingAveragesDF.iloc[index][\"BatterSide\"] + \"Batter\"\n",
    "    VisualUtil.visualizeData(averageProbs, [1], fileName)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for putting the right visuals on the correct excel sheets\n",
    "# For each player in the pitching averages, have a whole excel page for them\n",
    "import os\n",
    "importlib.reload(logs)\n",
    "print(pitchingAveragesDF)\n",
    "picList = []\n",
    "fileList = os.listdir(\"Visualization\")\n",
    "for x in pitchingAveragesDF[\"Pitcher\"].unique():\n",
    "    for y in fileList:\n",
    "        if x.replace(\",\", \"_\").replace(\" \", \"\") in y:\n",
    "            picList.append(y)\n",
    "    logs.writeToImageExcelSheet(picList,x)\n",
    "    picList = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
